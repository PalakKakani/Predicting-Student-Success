{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg():\n",
    "    train_path = \"/Users/user/pythonpractice/Omdena/Predicting Student Sucess Omdena Ankara 2023/train-student-mat.csv\"\n",
    "    test_path = \"/Users/user/pythonpractice/Omdena/Predicting Student Sucess Omdena Ankara 2023/test-student-mat.csv\"\n",
    "\n",
    "    test_size = 0.3\n",
    "\n",
    "    loss = \"mean_absolute_error\"\n",
    "\n",
    "    categorical_columns = ['school', 'sex', 'address', 'famsize', 'Pstatus',\n",
    "       'Mjob', 'Fjob', 'reason', 'guardian', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery',\n",
    "       'higher', 'internet', 'romantic']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def feature_engineering(path):\n",
    "    df = pd.read_csv(path)\n",
    "    dummies = pd.get_dummies(df.loc[:, cfg.categorical_columns])\n",
    "    df.drop(cfg.categorical_columns, axis=1, inplace=True)\n",
    "\n",
    "    df_output = pd.concat([df, dummies], levels=[\"id\"], axis=1)\n",
    "\n",
    "    df_output.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "\n",
    "    return df_output\n",
    "\n",
    "train = feature_engineering(cfg.train_path)\n",
    "test = feature_engineering(cfg.test_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'age', 'Medu', 'Fedu', 'traveltime', 'studytime', 'failures',\n",
       "       'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences',\n",
       "       'G1', 'G2', 'G3', 'Final_Score', 'school_GP', 'school_MS', 'sex_F',\n",
       "       'sex_M', 'address_R', 'address_U', 'famsize_GT3', 'famsize_LE3',\n",
       "       'Pstatus_A', 'Pstatus_T', 'Mjob_at_home', 'Mjob_health', 'Mjob_other',\n",
       "       'Mjob_services', 'Mjob_teacher', 'Fjob_at_home', 'Fjob_health',\n",
       "       'Fjob_other', 'Fjob_services', 'Fjob_teacher', 'reason_course',\n",
       "       'reason_home', 'reason_other', 'reason_reputation', 'guardian_father',\n",
       "       'guardian_mother', 'guardian_other', 'schoolsup_no', 'schoolsup_yes',\n",
       "       'famsup_no', 'famsup_yes', 'paid_no', 'paid_yes', 'activities_no',\n",
       "       'activities_yes', 'nursery_no', 'nursery_yes', 'higher_no',\n",
       "       'higher_yes', 'internet_no', 'internet_yes', 'romantic_no',\n",
       "       'romantic_yes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train = train.iloc[:, 1:]\n",
    "X_train.drop([\"G1\",\"G2\",\"G3\",\"Final_Score\"], axis=1, inplace=True)\n",
    "\n",
    "X_test = test.iloc[:, 1:]\n",
    "X_test.drop([\"G1\",\"G2\",\"G3\",\"Final_Score\"], axis=1, inplace=True)\n",
    "\n",
    "y_train = train.loc[:,[\"G1\",\"G2\",\"G3\",\"Final_Score\"]]\n",
    "y_test = test.loc[:,[\"G1\",\"G2\",\"G3\",\"Final_Score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "      <th>Final_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>11.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>17.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>277 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     G1  G2  G3  Final_Score\n",
       "0     5   5   6          5.4\n",
       "1     7   8  10          8.5\n",
       "2     6  10  10          8.8\n",
       "3    12  12  11         11.6\n",
       "4    16  18  19         17.8\n",
       "..   ..  ..  ..          ...\n",
       "272   7   5   0          3.6\n",
       "273   7   9   8          8.0\n",
       "274   6   5   0          3.3\n",
       "275  10   8   7          8.2\n",
       "276   8   9   9          8.7\n",
       "\n",
       "[277 rows x 4 columns]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg_dense():\n",
    "    \n",
    "    batch_size = 8\n",
    "    epochs = 160\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "model_dense = Sequential()\n",
    "model_dense.add(Dense(10, input_shape=(len(X_train.columns),), activation='relu'))\n",
    "model_dense.add(Dense(6, activation='relu'))\n",
    "model_dense.add(Dense(4, activation='relu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 0s 678us/step - loss: 9.4863\n",
      "Epoch 2/160\n",
      "35/35 [==============================] - 0s 620us/step - loss: 9.0381\n",
      "Epoch 3/160\n",
      "35/35 [==============================] - 0s 622us/step - loss: 8.9954\n",
      "Epoch 4/160\n",
      "35/35 [==============================] - 0s 598us/step - loss: 8.9917\n",
      "Epoch 5/160\n",
      "35/35 [==============================] - 0s 648us/step - loss: 8.9759\n",
      "Epoch 6/160\n",
      "35/35 [==============================] - 0s 607us/step - loss: 8.9692\n",
      "Epoch 7/160\n",
      "35/35 [==============================] - 0s 573us/step - loss: 8.9625\n",
      "Epoch 8/160\n",
      "35/35 [==============================] - 0s 552us/step - loss: 8.9556\n",
      "Epoch 9/160\n",
      "35/35 [==============================] - 0s 581us/step - loss: 8.9556\n",
      "Epoch 10/160\n",
      "35/35 [==============================] - 0s 542us/step - loss: 8.9402\n",
      "Epoch 11/160\n",
      "35/35 [==============================] - 0s 544us/step - loss: 8.9372\n",
      "Epoch 12/160\n",
      "35/35 [==============================] - 0s 530us/step - loss: 8.9309\n",
      "Epoch 13/160\n",
      "35/35 [==============================] - 0s 525us/step - loss: 8.9279\n",
      "Epoch 14/160\n",
      "35/35 [==============================] - 0s 542us/step - loss: 8.9297\n",
      "Epoch 15/160\n",
      "35/35 [==============================] - 0s 530us/step - loss: 8.9179\n",
      "Epoch 16/160\n",
      "35/35 [==============================] - 0s 522us/step - loss: 8.9125\n",
      "Epoch 17/160\n",
      "35/35 [==============================] - 0s 533us/step - loss: 8.9139\n",
      "Epoch 18/160\n",
      "35/35 [==============================] - 0s 528us/step - loss: 8.9052\n",
      "Epoch 19/160\n",
      "35/35 [==============================] - 0s 536us/step - loss: 8.9099\n",
      "Epoch 20/160\n",
      "35/35 [==============================] - 0s 535us/step - loss: 8.9073\n",
      "Epoch 21/160\n",
      "35/35 [==============================] - 0s 529us/step - loss: 8.8975\n",
      "Epoch 22/160\n",
      "35/35 [==============================] - 0s 526us/step - loss: 8.8947\n",
      "Epoch 23/160\n",
      "35/35 [==============================] - 0s 532us/step - loss: 8.8971\n",
      "Epoch 24/160\n",
      "35/35 [==============================] - 0s 524us/step - loss: 8.8875\n",
      "Epoch 25/160\n",
      "35/35 [==============================] - 0s 525us/step - loss: 8.8784\n",
      "Epoch 26/160\n",
      "35/35 [==============================] - 0s 532us/step - loss: 8.8894\n",
      "Epoch 27/160\n",
      "35/35 [==============================] - 0s 524us/step - loss: 8.8792\n",
      "Epoch 28/160\n",
      "35/35 [==============================] - 0s 521us/step - loss: 8.8750\n",
      "Epoch 29/160\n",
      "35/35 [==============================] - 0s 549us/step - loss: 8.8709\n",
      "Epoch 30/160\n",
      "35/35 [==============================] - 0s 544us/step - loss: 8.8691\n",
      "Epoch 31/160\n",
      "35/35 [==============================] - 0s 529us/step - loss: 8.8683\n",
      "Epoch 32/160\n",
      "35/35 [==============================] - 0s 526us/step - loss: 8.8617\n",
      "Epoch 33/160\n",
      "35/35 [==============================] - 0s 529us/step - loss: 8.8666\n",
      "Epoch 34/160\n",
      "35/35 [==============================] - 0s 526us/step - loss: 8.8659\n",
      "Epoch 35/160\n",
      "35/35 [==============================] - 0s 538us/step - loss: 8.8602\n",
      "Epoch 36/160\n",
      "35/35 [==============================] - 0s 529us/step - loss: 8.8600\n",
      "Epoch 37/160\n",
      "35/35 [==============================] - 0s 523us/step - loss: 8.8505\n",
      "Epoch 38/160\n",
      "35/35 [==============================] - 0s 524us/step - loss: 8.8517\n",
      "Epoch 39/160\n",
      "35/35 [==============================] - 0s 526us/step - loss: 8.8465\n",
      "Epoch 40/160\n",
      "35/35 [==============================] - 0s 526us/step - loss: 8.8495\n",
      "Epoch 41/160\n",
      "35/35 [==============================] - 0s 526us/step - loss: 8.8442\n",
      "Epoch 42/160\n",
      "35/35 [==============================] - 0s 528us/step - loss: 8.8416\n",
      "Epoch 43/160\n",
      "35/35 [==============================] - 0s 525us/step - loss: 8.8492\n",
      "Epoch 44/160\n",
      "35/35 [==============================] - 0s 527us/step - loss: 8.8402\n",
      "Epoch 45/160\n",
      "35/35 [==============================] - 0s 530us/step - loss: 8.8340\n",
      "Epoch 46/160\n",
      "35/35 [==============================] - 0s 523us/step - loss: 8.8365\n",
      "Epoch 47/160\n",
      "35/35 [==============================] - 0s 547us/step - loss: 8.8333\n",
      "Epoch 48/160\n",
      "35/35 [==============================] - 0s 562us/step - loss: 8.8410\n",
      "Epoch 49/160\n",
      "35/35 [==============================] - 0s 589us/step - loss: 8.8347\n",
      "Epoch 50/160\n",
      "35/35 [==============================] - 0s 592us/step - loss: 8.8289\n",
      "Epoch 51/160\n",
      "35/35 [==============================] - 0s 717us/step - loss: 8.8262\n",
      "Epoch 52/160\n",
      "35/35 [==============================] - 0s 632us/step - loss: 8.8370\n",
      "Epoch 53/160\n",
      "35/35 [==============================] - 0s 618us/step - loss: 8.8282\n",
      "Epoch 54/160\n",
      "35/35 [==============================] - 0s 626us/step - loss: 8.8231\n",
      "Epoch 55/160\n",
      "35/35 [==============================] - 0s 584us/step - loss: 8.8273\n",
      "Epoch 56/160\n",
      "35/35 [==============================] - 0s 651us/step - loss: 8.8175\n",
      "Epoch 57/160\n",
      "35/35 [==============================] - 0s 557us/step - loss: 8.8174\n",
      "Epoch 58/160\n",
      "35/35 [==============================] - 0s 618us/step - loss: 8.8144\n",
      "Epoch 59/160\n",
      "35/35 [==============================] - 0s 641us/step - loss: 8.8186\n",
      "Epoch 60/160\n",
      "35/35 [==============================] - 0s 651us/step - loss: 8.8187\n",
      "Epoch 61/160\n",
      "35/35 [==============================] - 0s 643us/step - loss: 8.8130\n",
      "Epoch 62/160\n",
      "35/35 [==============================] - 0s 613us/step - loss: 8.8099\n",
      "Epoch 63/160\n",
      "35/35 [==============================] - 0s 604us/step - loss: 8.8102\n",
      "Epoch 64/160\n",
      "35/35 [==============================] - 0s 670us/step - loss: 8.8068\n",
      "Epoch 65/160\n",
      "35/35 [==============================] - 0s 694us/step - loss: 8.8068\n",
      "Epoch 66/160\n",
      "35/35 [==============================] - 0s 733us/step - loss: 8.8063\n",
      "Epoch 67/160\n",
      "35/35 [==============================] - 0s 707us/step - loss: 8.8084\n",
      "Epoch 68/160\n",
      "35/35 [==============================] - 0s 925us/step - loss: 8.8064\n",
      "Epoch 69/160\n",
      "35/35 [==============================] - 0s 645us/step - loss: 8.7979\n",
      "Epoch 70/160\n",
      "35/35 [==============================] - 0s 726us/step - loss: 8.8042\n",
      "Epoch 71/160\n",
      "35/35 [==============================] - 0s 654us/step - loss: 8.7945\n",
      "Epoch 72/160\n",
      "35/35 [==============================] - 0s 935us/step - loss: 8.7974\n",
      "Epoch 73/160\n",
      "35/35 [==============================] - 0s 568us/step - loss: 8.7938\n",
      "Epoch 74/160\n",
      "35/35 [==============================] - 0s 549us/step - loss: 8.7950\n",
      "Epoch 75/160\n",
      "35/35 [==============================] - 0s 534us/step - loss: 8.7917\n",
      "Epoch 76/160\n",
      "35/35 [==============================] - 0s 522us/step - loss: 8.7904\n",
      "Epoch 77/160\n",
      "35/35 [==============================] - 0s 533us/step - loss: 8.7858\n",
      "Epoch 78/160\n",
      "35/35 [==============================] - 0s 525us/step - loss: 8.7987\n",
      "Epoch 79/160\n",
      "35/35 [==============================] - 0s 526us/step - loss: 8.7858\n",
      "Epoch 80/160\n",
      "35/35 [==============================] - 0s 526us/step - loss: 8.7847\n",
      "Epoch 81/160\n",
      "35/35 [==============================] - 0s 521us/step - loss: 8.7880\n",
      "Epoch 82/160\n",
      "35/35 [==============================] - 0s 531us/step - loss: 8.7800\n",
      "Epoch 83/160\n",
      "35/35 [==============================] - 0s 527us/step - loss: 8.7801\n",
      "Epoch 84/160\n",
      "35/35 [==============================] - 0s 526us/step - loss: 8.7760\n",
      "Epoch 85/160\n",
      "35/35 [==============================] - 0s 525us/step - loss: 8.7802\n",
      "Epoch 86/160\n",
      "35/35 [==============================] - 0s 528us/step - loss: 8.7750\n",
      "Epoch 87/160\n",
      "35/35 [==============================] - 0s 531us/step - loss: 8.7758\n",
      "Epoch 88/160\n",
      "35/35 [==============================] - 0s 542us/step - loss: 8.7753\n",
      "Epoch 89/160\n",
      "35/35 [==============================] - 0s 548us/step - loss: 8.7759\n",
      "Epoch 90/160\n",
      "35/35 [==============================] - 0s 529us/step - loss: 8.7714\n",
      "Epoch 91/160\n",
      "35/35 [==============================] - 0s 532us/step - loss: 8.7731\n",
      "Epoch 92/160\n",
      "35/35 [==============================] - 0s 533us/step - loss: 8.7815\n",
      "Epoch 93/160\n",
      "35/35 [==============================] - 0s 527us/step - loss: 8.7765\n",
      "Epoch 94/160\n",
      "35/35 [==============================] - 0s 519us/step - loss: 8.7723\n",
      "Epoch 95/160\n",
      "35/35 [==============================] - 0s 532us/step - loss: 8.7623\n",
      "Epoch 96/160\n",
      "35/35 [==============================] - 0s 532us/step - loss: 8.7591\n",
      "Epoch 97/160\n",
      "35/35 [==============================] - 0s 653us/step - loss: 8.7600\n",
      "Epoch 98/160\n",
      "35/35 [==============================] - 0s 847us/step - loss: 8.7658\n",
      "Epoch 99/160\n",
      "35/35 [==============================] - 0s 533us/step - loss: 8.7667\n",
      "Epoch 100/160\n",
      "35/35 [==============================] - 0s 531us/step - loss: 8.7593\n",
      "Epoch 101/160\n",
      "35/35 [==============================] - 0s 518us/step - loss: 8.7516\n",
      "Epoch 102/160\n",
      "35/35 [==============================] - 0s 528us/step - loss: 8.7562\n",
      "Epoch 103/160\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 8.7679\n",
      "Epoch 104/160\n",
      "35/35 [==============================] - 0s 796us/step - loss: 8.7640\n",
      "Epoch 105/160\n",
      "35/35 [==============================] - 0s 527us/step - loss: 8.7543\n",
      "Epoch 106/160\n",
      "35/35 [==============================] - 0s 530us/step - loss: 8.7543\n",
      "Epoch 107/160\n",
      "35/35 [==============================] - 0s 529us/step - loss: 8.7478\n",
      "Epoch 108/160\n",
      "35/35 [==============================] - 0s 525us/step - loss: 8.7489\n",
      "Epoch 109/160\n",
      "35/35 [==============================] - 0s 540us/step - loss: 8.7454\n",
      "Epoch 110/160\n",
      "35/35 [==============================] - 0s 531us/step - loss: 8.7454\n",
      "Epoch 111/160\n",
      "35/35 [==============================] - 0s 527us/step - loss: 8.7488\n",
      "Epoch 112/160\n",
      "35/35 [==============================] - 0s 524us/step - loss: 8.7481\n",
      "Epoch 113/160\n",
      "35/35 [==============================] - 0s 526us/step - loss: 8.7464\n",
      "Epoch 114/160\n",
      "35/35 [==============================] - 0s 527us/step - loss: 8.7352\n",
      "Epoch 115/160\n",
      "35/35 [==============================] - 0s 526us/step - loss: 8.7402\n",
      "Epoch 116/160\n",
      "35/35 [==============================] - 0s 526us/step - loss: 8.7361\n",
      "Epoch 117/160\n",
      "35/35 [==============================] - 0s 527us/step - loss: 8.7348\n",
      "Epoch 118/160\n",
      "35/35 [==============================] - 0s 529us/step - loss: 8.7354\n",
      "Epoch 119/160\n",
      "35/35 [==============================] - 0s 535us/step - loss: 8.7334\n",
      "Epoch 120/160\n",
      "35/35 [==============================] - 0s 533us/step - loss: 8.7284\n",
      "Epoch 121/160\n",
      "35/35 [==============================] - 0s 524us/step - loss: 8.7265\n",
      "Epoch 122/160\n",
      "35/35 [==============================] - 0s 523us/step - loss: 8.7389\n",
      "Epoch 123/160\n",
      "35/35 [==============================] - 0s 519us/step - loss: 8.7296\n",
      "Epoch 124/160\n",
      "35/35 [==============================] - 0s 526us/step - loss: 8.7282\n",
      "Epoch 125/160\n",
      "35/35 [==============================] - 0s 528us/step - loss: 8.7197\n",
      "Epoch 126/160\n",
      "35/35 [==============================] - 0s 525us/step - loss: 8.7284\n",
      "Epoch 127/160\n",
      "35/35 [==============================] - 0s 525us/step - loss: 8.7226\n",
      "Epoch 128/160\n",
      "35/35 [==============================] - 0s 526us/step - loss: 8.7125\n",
      "Epoch 129/160\n",
      "35/35 [==============================] - 0s 513us/step - loss: 8.7138\n",
      "Epoch 130/160\n",
      "35/35 [==============================] - 0s 534us/step - loss: 8.7085\n",
      "Epoch 131/160\n",
      "35/35 [==============================] - 0s 520us/step - loss: 8.7104\n",
      "Epoch 132/160\n",
      "35/35 [==============================] - 0s 529us/step - loss: 8.7087\n",
      "Epoch 133/160\n",
      "35/35 [==============================] - 0s 528us/step - loss: 8.7093\n",
      "Epoch 134/160\n",
      "35/35 [==============================] - 0s 535us/step - loss: 8.7066\n",
      "Epoch 135/160\n",
      "35/35 [==============================] - 0s 523us/step - loss: 8.7043\n",
      "Epoch 136/160\n",
      "35/35 [==============================] - 0s 529us/step - loss: 8.7021\n",
      "Epoch 137/160\n",
      "35/35 [==============================] - 0s 529us/step - loss: 8.6976\n",
      "Epoch 138/160\n",
      "35/35 [==============================] - 0s 523us/step - loss: 8.7059\n",
      "Epoch 139/160\n",
      "35/35 [==============================] - 0s 529us/step - loss: 8.6923\n",
      "Epoch 140/160\n",
      "35/35 [==============================] - 0s 529us/step - loss: 8.6968\n",
      "Epoch 141/160\n",
      "35/35 [==============================] - 0s 524us/step - loss: 8.6927\n",
      "Epoch 142/160\n",
      "35/35 [==============================] - 0s 530us/step - loss: 8.6929\n",
      "Epoch 143/160\n",
      "35/35 [==============================] - 0s 529us/step - loss: 8.6865\n",
      "Epoch 144/160\n",
      "35/35 [==============================] - 0s 530us/step - loss: 8.6850\n",
      "Epoch 145/160\n",
      "35/35 [==============================] - 0s 525us/step - loss: 8.6850\n",
      "Epoch 146/160\n",
      "35/35 [==============================] - 0s 533us/step - loss: 8.6916\n",
      "Epoch 147/160\n",
      "35/35 [==============================] - 0s 522us/step - loss: 8.6857\n",
      "Epoch 148/160\n",
      "35/35 [==============================] - 0s 524us/step - loss: 8.6984\n",
      "Epoch 149/160\n",
      "35/35 [==============================] - 0s 518us/step - loss: 8.6758\n",
      "Epoch 150/160\n",
      "35/35 [==============================] - 0s 525us/step - loss: 8.6799\n",
      "Epoch 151/160\n",
      "35/35 [==============================] - 0s 519us/step - loss: 8.6740\n",
      "Epoch 152/160\n",
      "35/35 [==============================] - 0s 526us/step - loss: 8.6701\n",
      "Epoch 153/160\n",
      "35/35 [==============================] - 0s 525us/step - loss: 8.6741\n",
      "Epoch 154/160\n",
      "35/35 [==============================] - 0s 517us/step - loss: 8.6776\n",
      "Epoch 155/160\n",
      "35/35 [==============================] - 0s 527us/step - loss: 8.6674\n",
      "Epoch 156/160\n",
      "35/35 [==============================] - 0s 528us/step - loss: 8.6749\n",
      "Epoch 157/160\n",
      "35/35 [==============================] - 0s 530us/step - loss: 8.6756\n",
      "Epoch 158/160\n",
      "35/35 [==============================] - 0s 520us/step - loss: 8.6692\n",
      "Epoch 159/160\n",
      "35/35 [==============================] - 0s 525us/step - loss: 8.6640\n",
      "Epoch 160/160\n",
      "35/35 [==============================] - 0s 519us/step - loss: 8.6590\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17ad566e0>"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dense.compile(loss=cfg.loss, optimizer=\"Adam\")\n",
    "model_dense.fit(X_train, y_train, epochs=cfg_dense.epochs, batch_size=cfg_dense.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 798us/step\n",
      "8.781063617033475\n"
     ]
    }
   ],
   "source": [
    "y_pred_dense = model_dense.predict(X_test)\n",
    "\n",
    "print(mean_absolute_error(y_test, y_pred_dense))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.       ,  0.       ,  1.444501 ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  7.257052 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 11.776026 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 10.759888 ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  8.392878 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 10.857268 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 14.541388 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 11.009794 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 10.206882 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 12.891474 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 12.756036 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 11.692886 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 12.609497 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 14.476685 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 12.204055 ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  8.8682375,  0.       ],\n",
       "       [ 0.       ,  0.       , 11.3741865,  0.       ],\n",
       "       [ 0.       ,  0.       ,  8.9753275,  0.       ],\n",
       "       [ 0.       ,  0.       ,  9.771464 ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  8.195663 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 11.931295 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 13.229398 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 14.92387  ,  0.       ],\n",
       "       [ 0.       ,  0.       , 13.9      ,  0.       ],\n",
       "       [ 0.       ,  0.       , 10.603564 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 11.727829 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 13.753777 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 13.995749 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 14.680921 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 12.113123 ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  3.8758461,  0.       ],\n",
       "       [ 0.       ,  0.       ,  8.463018 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 15.327052 ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  7.821927 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 11.1050825,  0.       ],\n",
       "       [ 0.       ,  0.       ,  7.672751 ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  3.3834465,  0.       ],\n",
       "       [ 0.       ,  0.       , 11.4889345,  0.       ],\n",
       "       [ 0.       ,  0.       , 12.689571 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 10.128608 ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  3.109652 ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  9.780031 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 14.93009  ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  5.989551 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 10.469707 ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  7.8238688,  0.       ],\n",
       "       [ 0.       ,  0.       , 10.536057 ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  8.922947 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 10.132297 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 17.072731 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 10.834609 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 12.394499 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 16.450588 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 10.984789 ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  7.9110484,  0.       ],\n",
       "       [ 0.       ,  0.       , 10.5628805,  0.       ],\n",
       "       [ 0.       ,  0.       , 14.887704 ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  8.250686 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 14.475435 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 14.207437 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 13.464416 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 12.536701 ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  8.735837 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 14.4046545,  0.       ],\n",
       "       [ 0.       ,  0.       ,  7.626622 ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  9.888565 ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  9.548794 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 13.724928 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 12.2697315,  0.       ],\n",
       "       [ 0.       ,  0.       , 11.422187 ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  9.403942 ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  8.75037  ,  0.       ],\n",
       "       [ 0.       ,  0.       , 12.075776 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 15.120024 ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  8.6767235,  0.       ],\n",
       "       [ 0.       ,  0.       , 11.00596  ,  0.       ],\n",
       "       [ 0.       ,  0.       , 15.569204 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 13.457924 ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  5.744264 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 14.374326 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 10.535996 ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  4.8301077,  0.       ],\n",
       "       [ 0.       ,  0.       ,  9.901223 ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  9.229678 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 11.102834 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 10.892595 ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  5.6573076,  0.       ],\n",
       "       [ 0.       ,  0.       , 15.090372 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 14.430054 ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  7.227823 ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  9.751706 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 11.734881 ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  7.2760944,  0.       ],\n",
       "       [ 0.       ,  0.       ,  8.3305645,  0.       ],\n",
       "       [ 0.       ,  0.       ,  5.650396 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 13.480255 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 10.115315 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 12.0910015,  0.       ],\n",
       "       [ 0.       ,  0.       , 10.834874 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 13.358733 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 14.070999 ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  8.786266 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 10.886765 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 14.172483 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 16.703339 ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  7.3503165,  0.       ],\n",
       "       [ 0.       ,  0.       ,  8.910908 ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  8.560984 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 12.168756 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 12.1450205,  0.       ],\n",
       "       [ 0.       ,  0.       , 12.498599 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 11.296519 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 11.630645 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 11.592024 ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  6.269446 ,  0.       ],\n",
       "       [ 0.       ,  0.       , 12.982772 ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  5.97349  ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  7.7320585,  0.       ]], dtype=float32)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "      <th>Final_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>118 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     G1  G2  G3  Final_Score\n",
       "0     8   8  10          8.8\n",
       "1    14  12  12         12.6\n",
       "2     3   5   5          4.4\n",
       "3     8   9  10          9.1\n",
       "4     9   9   9          9.0\n",
       "..   ..  ..  ..          ...\n",
       "113   5   5   5          5.0\n",
       "114  11   9  10         10.0\n",
       "115  18  18  18         18.0\n",
       "116   6   8   8          7.4\n",
       "117   7   7   8          7.4\n",
       "\n",
       "[118 rows x 4 columns]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense model on only final score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "model_dense = Sequential()\n",
    "model_dense.add(Dense(6, input_shape=(len(X_train.columns),), activation='relu'))\n",
    "model_dense.add(Dense(3, activation='relu'))\n",
    "model_dense.add(Dense(1, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_final = y_train.loc[:,[\"Final_Score\"]]\n",
    "y_test_final = y_test.loc[:,[\"Final_Score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 0s 645us/step - loss: 10.3361\n",
      "Epoch 2/160\n",
      "35/35 [==============================] - 0s 566us/step - loss: 7.8365\n",
      "Epoch 3/160\n",
      "35/35 [==============================] - 0s 550us/step - loss: 5.0309\n",
      "Epoch 4/160\n",
      "35/35 [==============================] - 0s 524us/step - loss: 3.5687\n",
      "Epoch 5/160\n",
      "35/35 [==============================] - 0s 527us/step - loss: 3.2527\n",
      "Epoch 6/160\n",
      "35/35 [==============================] - 0s 571us/step - loss: 3.1799\n",
      "Epoch 7/160\n",
      "35/35 [==============================] - 0s 527us/step - loss: 3.1384\n",
      "Epoch 8/160\n",
      "35/35 [==============================] - 0s 528us/step - loss: 3.0986\n",
      "Epoch 9/160\n",
      "35/35 [==============================] - 0s 530us/step - loss: 3.0565\n",
      "Epoch 10/160\n",
      "35/35 [==============================] - 0s 523us/step - loss: 3.0295\n",
      "Epoch 11/160\n",
      "35/35 [==============================] - 0s 547us/step - loss: 2.9879\n",
      "Epoch 12/160\n",
      "35/35 [==============================] - 0s 528us/step - loss: 2.9655\n",
      "Epoch 13/160\n",
      "35/35 [==============================] - 0s 533us/step - loss: 2.9283\n",
      "Epoch 14/160\n",
      "35/35 [==============================] - 0s 522us/step - loss: 2.8989\n",
      "Epoch 15/160\n",
      "35/35 [==============================] - 0s 528us/step - loss: 2.8746\n",
      "Epoch 16/160\n",
      "35/35 [==============================] - 0s 528us/step - loss: 2.8545\n",
      "Epoch 17/160\n",
      "35/35 [==============================] - 0s 526us/step - loss: 2.8271\n",
      "Epoch 18/160\n",
      "35/35 [==============================] - 0s 521us/step - loss: 2.8170\n",
      "Epoch 19/160\n",
      "35/35 [==============================] - 0s 518us/step - loss: 2.7889\n",
      "Epoch 20/160\n",
      "35/35 [==============================] - 0s 519us/step - loss: 2.7629\n",
      "Epoch 21/160\n",
      "35/35 [==============================] - 0s 516us/step - loss: 2.7428\n",
      "Epoch 22/160\n",
      "35/35 [==============================] - 0s 528us/step - loss: 2.7383\n",
      "Epoch 23/160\n",
      "35/35 [==============================] - 0s 521us/step - loss: 2.7217\n",
      "Epoch 24/160\n",
      "35/35 [==============================] - 0s 519us/step - loss: 2.6952\n",
      "Epoch 25/160\n",
      "35/35 [==============================] - 0s 517us/step - loss: 2.6739\n",
      "Epoch 26/160\n",
      "35/35 [==============================] - 0s 514us/step - loss: 2.6518\n",
      "Epoch 27/160\n",
      "35/35 [==============================] - 0s 513us/step - loss: 2.6317\n",
      "Epoch 28/160\n",
      "35/35 [==============================] - 0s 520us/step - loss: 2.6233\n",
      "Epoch 29/160\n",
      "35/35 [==============================] - 0s 520us/step - loss: 2.6190\n",
      "Epoch 30/160\n",
      "35/35 [==============================] - 0s 527us/step - loss: 2.5935\n",
      "Epoch 31/160\n",
      "35/35 [==============================] - 0s 519us/step - loss: 2.5844\n",
      "Epoch 32/160\n",
      "35/35 [==============================] - 0s 538us/step - loss: 2.6072\n",
      "Epoch 33/160\n",
      "35/35 [==============================] - 0s 527us/step - loss: 2.5718\n",
      "Epoch 34/160\n",
      "35/35 [==============================] - 0s 570us/step - loss: 2.5562\n",
      "Epoch 35/160\n",
      "35/35 [==============================] - 0s 617us/step - loss: 2.5546\n",
      "Epoch 36/160\n",
      "35/35 [==============================] - 0s 575us/step - loss: 2.5328\n",
      "Epoch 37/160\n",
      "35/35 [==============================] - 0s 550us/step - loss: 2.5355\n",
      "Epoch 38/160\n",
      "35/35 [==============================] - 0s 550us/step - loss: 2.5130\n",
      "Epoch 39/160\n",
      "35/35 [==============================] - 0s 529us/step - loss: 2.4958\n",
      "Epoch 40/160\n",
      "35/35 [==============================] - 0s 532us/step - loss: 2.4957\n",
      "Epoch 41/160\n",
      "35/35 [==============================] - 0s 536us/step - loss: 2.4799\n",
      "Epoch 42/160\n",
      "35/35 [==============================] - 0s 520us/step - loss: 2.4682\n",
      "Epoch 43/160\n",
      "35/35 [==============================] - 0s 517us/step - loss: 2.4672\n",
      "Epoch 44/160\n",
      "35/35 [==============================] - 0s 577us/step - loss: 2.4650\n",
      "Epoch 45/160\n",
      "35/35 [==============================] - 0s 676us/step - loss: 2.4527\n",
      "Epoch 46/160\n",
      "35/35 [==============================] - 0s 682us/step - loss: 2.4501\n",
      "Epoch 47/160\n",
      "35/35 [==============================] - 0s 669us/step - loss: 2.4312\n",
      "Epoch 48/160\n",
      "35/35 [==============================] - 0s 659us/step - loss: 2.4454\n",
      "Epoch 49/160\n",
      "35/35 [==============================] - 0s 598us/step - loss: 2.4459\n",
      "Epoch 50/160\n",
      "35/35 [==============================] - 0s 556us/step - loss: 2.4261\n",
      "Epoch 51/160\n",
      "35/35 [==============================] - 0s 574us/step - loss: 2.4277\n",
      "Epoch 52/160\n",
      "35/35 [==============================] - 0s 580us/step - loss: 2.4306\n",
      "Epoch 53/160\n",
      "35/35 [==============================] - 0s 596us/step - loss: 2.4303\n",
      "Epoch 54/160\n",
      "35/35 [==============================] - 0s 789us/step - loss: 2.4017\n",
      "Epoch 55/160\n",
      "35/35 [==============================] - 0s 671us/step - loss: 2.4142\n",
      "Epoch 56/160\n",
      "35/35 [==============================] - 0s 626us/step - loss: 2.3916\n",
      "Epoch 57/160\n",
      "35/35 [==============================] - 0s 640us/step - loss: 2.4110\n",
      "Epoch 58/160\n",
      "35/35 [==============================] - 0s 633us/step - loss: 2.3868\n",
      "Epoch 59/160\n",
      "35/35 [==============================] - 0s 636us/step - loss: 2.4120\n",
      "Epoch 60/160\n",
      "35/35 [==============================] - 0s 660us/step - loss: 2.3853\n",
      "Epoch 61/160\n",
      "35/35 [==============================] - 0s 537us/step - loss: 2.4082\n",
      "Epoch 62/160\n",
      "35/35 [==============================] - 0s 533us/step - loss: 2.3818\n",
      "Epoch 63/160\n",
      "35/35 [==============================] - 0s 517us/step - loss: 2.3762\n",
      "Epoch 64/160\n",
      "35/35 [==============================] - 0s 520us/step - loss: 2.3786\n",
      "Epoch 65/160\n",
      "35/35 [==============================] - 0s 523us/step - loss: 2.3706\n",
      "Epoch 66/160\n",
      "35/35 [==============================] - 0s 531us/step - loss: 2.3665\n",
      "Epoch 67/160\n",
      "35/35 [==============================] - 0s 526us/step - loss: 2.3705\n",
      "Epoch 68/160\n",
      "35/35 [==============================] - 0s 520us/step - loss: 2.3879\n",
      "Epoch 69/160\n",
      "35/35 [==============================] - 0s 519us/step - loss: 2.3610\n",
      "Epoch 70/160\n",
      "35/35 [==============================] - 0s 529us/step - loss: 2.3583\n",
      "Epoch 71/160\n",
      "35/35 [==============================] - 0s 524us/step - loss: 2.3523\n",
      "Epoch 72/160\n",
      "35/35 [==============================] - 0s 523us/step - loss: 2.3534\n",
      "Epoch 73/160\n",
      "35/35 [==============================] - 0s 523us/step - loss: 2.3590\n",
      "Epoch 74/160\n",
      "35/35 [==============================] - 0s 523us/step - loss: 2.3469\n",
      "Epoch 75/160\n",
      "35/35 [==============================] - 0s 524us/step - loss: 2.3483\n",
      "Epoch 76/160\n",
      "35/35 [==============================] - 0s 535us/step - loss: 2.3518\n",
      "Epoch 77/160\n",
      "35/35 [==============================] - 0s 570us/step - loss: 2.3453\n",
      "Epoch 78/160\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 2.3385\n",
      "Epoch 79/160\n",
      "35/35 [==============================] - 0s 993us/step - loss: 2.3671\n",
      "Epoch 80/160\n",
      "35/35 [==============================] - 0s 675us/step - loss: 2.3356\n",
      "Epoch 81/160\n",
      "35/35 [==============================] - 0s 670us/step - loss: 2.3361\n",
      "Epoch 82/160\n",
      "35/35 [==============================] - 0s 576us/step - loss: 2.3734\n",
      "Epoch 83/160\n",
      "35/35 [==============================] - 0s 571us/step - loss: 2.3458\n",
      "Epoch 84/160\n",
      "35/35 [==============================] - 0s 543us/step - loss: 2.3330\n",
      "Epoch 85/160\n",
      "35/35 [==============================] - 0s 525us/step - loss: 2.3278\n",
      "Epoch 86/160\n",
      "35/35 [==============================] - 0s 524us/step - loss: 2.3436\n",
      "Epoch 87/160\n",
      "35/35 [==============================] - 0s 521us/step - loss: 2.3407\n",
      "Epoch 88/160\n",
      "35/35 [==============================] - 0s 531us/step - loss: 2.3217\n",
      "Epoch 89/160\n",
      "35/35 [==============================] - 0s 533us/step - loss: 2.3240\n",
      "Epoch 90/160\n",
      "35/35 [==============================] - 0s 520us/step - loss: 2.3179\n",
      "Epoch 91/160\n",
      "35/35 [==============================] - 0s 517us/step - loss: 2.3264\n",
      "Epoch 92/160\n",
      "35/35 [==============================] - 0s 519us/step - loss: 2.3463\n",
      "Epoch 93/160\n",
      "35/35 [==============================] - 0s 546us/step - loss: 2.3303\n",
      "Epoch 94/160\n",
      "35/35 [==============================] - 0s 563us/step - loss: 2.3113\n",
      "Epoch 95/160\n",
      "35/35 [==============================] - 0s 536us/step - loss: 2.3310\n",
      "Epoch 96/160\n",
      "35/35 [==============================] - 0s 521us/step - loss: 2.3131\n",
      "Epoch 97/160\n",
      "35/35 [==============================] - 0s 533us/step - loss: 2.3155\n",
      "Epoch 98/160\n",
      "35/35 [==============================] - 0s 530us/step - loss: 2.2997\n",
      "Epoch 99/160\n",
      "35/35 [==============================] - 0s 532us/step - loss: 2.3063\n",
      "Epoch 100/160\n",
      "35/35 [==============================] - 0s 518us/step - loss: 2.3025\n",
      "Epoch 101/160\n",
      "35/35 [==============================] - 0s 523us/step - loss: 2.3024\n",
      "Epoch 102/160\n",
      "35/35 [==============================] - 0s 523us/step - loss: 2.3054\n",
      "Epoch 103/160\n",
      "35/35 [==============================] - 0s 536us/step - loss: 2.3144\n",
      "Epoch 104/160\n",
      "35/35 [==============================] - 0s 536us/step - loss: 2.3066\n",
      "Epoch 105/160\n",
      "35/35 [==============================] - 0s 547us/step - loss: 2.2834\n",
      "Epoch 106/160\n",
      "35/35 [==============================] - 0s 541us/step - loss: 2.2877\n",
      "Epoch 107/160\n",
      "35/35 [==============================] - 0s 546us/step - loss: 2.2908\n",
      "Epoch 108/160\n",
      "35/35 [==============================] - 0s 560us/step - loss: 2.2895\n",
      "Epoch 109/160\n",
      "35/35 [==============================] - 0s 533us/step - loss: 2.2801\n",
      "Epoch 110/160\n",
      "35/35 [==============================] - 0s 544us/step - loss: 2.2841\n",
      "Epoch 111/160\n",
      "35/35 [==============================] - 0s 570us/step - loss: 2.2941\n",
      "Epoch 112/160\n",
      "35/35 [==============================] - 0s 593us/step - loss: 2.3105\n",
      "Epoch 113/160\n",
      "35/35 [==============================] - 0s 578us/step - loss: 2.2846\n",
      "Epoch 114/160\n",
      "35/35 [==============================] - 0s 570us/step - loss: 2.2980\n",
      "Epoch 115/160\n",
      "35/35 [==============================] - 0s 581us/step - loss: 2.2856\n",
      "Epoch 116/160\n",
      "35/35 [==============================] - 0s 587us/step - loss: 2.3079\n",
      "Epoch 117/160\n",
      "35/35 [==============================] - 0s 565us/step - loss: 2.3139\n",
      "Epoch 118/160\n",
      "35/35 [==============================] - 0s 562us/step - loss: 2.2893\n",
      "Epoch 119/160\n",
      "35/35 [==============================] - 0s 539us/step - loss: 2.3047\n",
      "Epoch 120/160\n",
      "35/35 [==============================] - 0s 529us/step - loss: 2.2934\n",
      "Epoch 121/160\n",
      "35/35 [==============================] - 0s 531us/step - loss: 2.2840\n",
      "Epoch 122/160\n",
      "35/35 [==============================] - 0s 531us/step - loss: 2.2654\n",
      "Epoch 123/160\n",
      "35/35 [==============================] - 0s 531us/step - loss: 2.2915\n",
      "Epoch 124/160\n",
      "35/35 [==============================] - 0s 529us/step - loss: 2.2766\n",
      "Epoch 125/160\n",
      "35/35 [==============================] - 0s 524us/step - loss: 2.2741\n",
      "Epoch 126/160\n",
      "35/35 [==============================] - 0s 525us/step - loss: 2.2622\n",
      "Epoch 127/160\n",
      "35/35 [==============================] - 0s 522us/step - loss: 2.2704\n",
      "Epoch 128/160\n",
      "35/35 [==============================] - 0s 536us/step - loss: 2.2608\n",
      "Epoch 129/160\n",
      "35/35 [==============================] - 0s 524us/step - loss: 2.2567\n",
      "Epoch 130/160\n",
      "35/35 [==============================] - 0s 521us/step - loss: 2.2607\n",
      "Epoch 131/160\n",
      "35/35 [==============================] - 0s 523us/step - loss: 2.2667\n",
      "Epoch 132/160\n",
      "35/35 [==============================] - 0s 516us/step - loss: 2.2740\n",
      "Epoch 133/160\n",
      "35/35 [==============================] - 0s 522us/step - loss: 2.2655\n",
      "Epoch 134/160\n",
      "35/35 [==============================] - 0s 521us/step - loss: 2.2661\n",
      "Epoch 135/160\n",
      "35/35 [==============================] - 0s 523us/step - loss: 2.2662\n",
      "Epoch 136/160\n",
      "35/35 [==============================] - 0s 520us/step - loss: 2.2607\n",
      "Epoch 137/160\n",
      "35/35 [==============================] - 0s 524us/step - loss: 2.2617\n",
      "Epoch 138/160\n",
      "35/35 [==============================] - 0s 520us/step - loss: 2.2646\n",
      "Epoch 139/160\n",
      "35/35 [==============================] - 0s 520us/step - loss: 2.2709\n",
      "Epoch 140/160\n",
      "35/35 [==============================] - 0s 521us/step - loss: 2.2446\n",
      "Epoch 141/160\n",
      "35/35 [==============================] - 0s 524us/step - loss: 2.2630\n",
      "Epoch 142/160\n",
      "35/35 [==============================] - 0s 525us/step - loss: 2.2514\n",
      "Epoch 143/160\n",
      "35/35 [==============================] - 0s 524us/step - loss: 2.2419\n",
      "Epoch 144/160\n",
      "35/35 [==============================] - 0s 537us/step - loss: 2.2513\n",
      "Epoch 145/160\n",
      "35/35 [==============================] - 0s 524us/step - loss: 2.2320\n",
      "Epoch 146/160\n",
      "35/35 [==============================] - 0s 525us/step - loss: 2.2440\n",
      "Epoch 147/160\n",
      "35/35 [==============================] - 0s 519us/step - loss: 2.2568\n",
      "Epoch 148/160\n",
      "35/35 [==============================] - 0s 526us/step - loss: 2.2432\n",
      "Epoch 149/160\n",
      "35/35 [==============================] - 0s 520us/step - loss: 2.2331\n",
      "Epoch 150/160\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 2.2412\n",
      "Epoch 151/160\n",
      "35/35 [==============================] - 0s 836us/step - loss: 2.2869\n",
      "Epoch 152/160\n",
      "35/35 [==============================] - 0s 575us/step - loss: 2.2340\n",
      "Epoch 153/160\n",
      "35/35 [==============================] - 0s 529us/step - loss: 2.2281\n",
      "Epoch 154/160\n",
      "35/35 [==============================] - 0s 519us/step - loss: 2.2307\n",
      "Epoch 155/160\n",
      "35/35 [==============================] - 0s 521us/step - loss: 2.2297\n",
      "Epoch 156/160\n",
      "35/35 [==============================] - 0s 525us/step - loss: 2.2476\n",
      "Epoch 157/160\n",
      "35/35 [==============================] - 0s 517us/step - loss: 2.2505\n",
      "Epoch 158/160\n",
      "35/35 [==============================] - 0s 521us/step - loss: 2.2286\n",
      "Epoch 159/160\n",
      "35/35 [==============================] - 0s 522us/step - loss: 2.2231\n",
      "Epoch 160/160\n",
      "35/35 [==============================] - 0s 521us/step - loss: 2.2232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17b5922f0>"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dense.compile(loss=cfg.loss, optimizer=\"Adam\")\n",
    "model_dense.fit(X_train, y_train_final, epochs=cfg_dense.epochs, batch_size=cfg_dense.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 762us/step\n",
      "2.8571277765904446\n"
     ]
    }
   ],
   "source": [
    "y_pred_dense_final = model_dense.predict(X_test)\n",
    "\n",
    "print(mean_absolute_error(y_test_final, y_pred_dense_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "class cfg_grad():\n",
    "\n",
    "    params = {\n",
    "    \"n_estimators\": 40,\n",
    "    \"max_depth\": 6,\n",
    "    \"learning_rate\": 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-11 {color: black;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "             importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "             learning_rate=0.1, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "             missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=40, n_jobs=0,\n",
       "             num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" checked><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "             importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "             learning_rate=0.1, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "             missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=40, n_jobs=0,\n",
       "             num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "             importance_type=None, interaction_constraints='',\n",
       "             learning_rate=0.1, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "             missing=nan, monotone_constraints='()', n_estimators=40, n_jobs=0,\n",
       "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, ...)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_grad = XGBRegressor(**cfg_grad.params)\n",
    "model_grad.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8070644408969554\n"
     ]
    }
   ],
   "source": [
    "y_pred_grad = model_grad.predict(X_test)\n",
    "\n",
    "print(mean_absolute_error(y_test, y_pred_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failures: 0.09730807691812515\n",
      "guardian_other: 0.05173223465681076\n",
      "absences: 0.04496052488684654\n",
      "Pstatus_A: 0.035383351147174835\n",
      "Fjob_teacher: 0.034269675612449646\n",
      "higher_no: 0.0325729101896286\n",
      "reason_home: 0.03082229010760784\n",
      "internet_no: 0.028541946783661842\n",
      "Fjob_at_home: 0.028166165575385094\n",
      "goout: 0.02778368815779686\n",
      "schoolsup_no: 0.026972096413373947\n",
      "health: 0.026461122557520866\n",
      "reason_course: 0.02643711306154728\n",
      "Mjob_at_home: 0.02453944832086563\n",
      "Mjob_health: 0.023739829659461975\n",
      "sex_F: 0.02246224321424961\n",
      "paid_no: 0.02204476110637188\n",
      "guardian_mother: 0.020011603832244873\n",
      "nursery_no: 0.019827023148536682\n",
      "traveltime: 0.019455116242170334\n",
      "Walc: 0.019406074658036232\n",
      "studytime: 0.019220972433686256\n",
      "reason_reputation: 0.018613986670970917\n",
      "Fedu: 0.01825936883687973\n",
      "famrel: 0.018078183755278587\n",
      "Fjob_other: 0.017299026250839233\n",
      "Mjob_services: 0.017242230474948883\n",
      "activities_no: 0.016760028898715973\n",
      "Mjob_other: 0.01644357666373253\n",
      "Dalc: 0.01637553982436657\n",
      "Mjob_teacher: 0.01626225747168064\n",
      "famsize_GT3: 0.016016172245144844\n",
      "romantic_no: 0.016011690720915794\n",
      "reason_other: 0.015922751277685165\n",
      "famsup_no: 0.01506203506141901\n",
      "freetime: 0.014718132093548775\n",
      "address_R: 0.014299790374934673\n",
      "school_GP: 0.014213358983397484\n",
      "guardian_father: 0.013656585477292538\n",
      "Fjob_services: 0.013042093254625797\n",
      "age: 0.011902031488716602\n",
      "Medu: 0.011052302084863186\n",
      "Fjob_health: 0.006650582887232304\n",
      "sex_M: 0.0\n",
      "schoolsup_yes: 0.0\n",
      "school_MS: 0.0\n",
      "romantic_yes: 0.0\n",
      "paid_yes: 0.0\n",
      "nursery_yes: 0.0\n",
      "internet_yes: 0.0\n",
      "higher_yes: 0.0\n",
      "famsup_yes: 0.0\n",
      "famsize_LE3: 0.0\n",
      "address_U: 0.0\n",
      "activities_yes: 0.0\n",
      "Pstatus_T: 0.0\n"
     ]
    }
   ],
   "source": [
    "grad_importances = model_grad.feature_importances_\n",
    "\n",
    "sorted_importances = sorted(zip(grad_importances, X_test.columns), reverse=True)\n",
    "\n",
    "for importance, name in sorted_importances:\n",
    "    print(f\"{name}: {importance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0) Using the feature engineering methodology done by the EDA Team 1\n",
    "1) Normalization of features \n",
    "2) Feature selection \n",
    "3) New feature creations\n",
    "4) Hyperparameter tuning\n",
    "5) Various models \n",
    "6) Various ensembles\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
